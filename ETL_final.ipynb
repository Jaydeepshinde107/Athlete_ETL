{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557ab54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docutils.nodes import header\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.functions import col, regexp_replace, split, round\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfe133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------   write data from local, into HDFS   -----------------------------------\n",
    "\n",
    "# Define the local file path and the HDFS path\n",
    "local_athlete = \"file:///home/talentum/test-jupyter/test/MiniProject/athlete_events.csv\"\n",
    "hdfs_athlete = \"athlete_events.csv\"\n",
    "\n",
    "local_winners = \"file:///home/talentum/test-jupyter/test/MiniProject/Winners.csv\"\n",
    "hdfs_winners = \"Winners.csv\"\n",
    "\n",
    "local_noc = \"file:///home/talentum/test-jupyter/test/MiniProject/noc_regions.csv\"\n",
    "hdfs_noc = \"noc.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f91deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "athlete_schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Sex\", StringType(), True),\n",
    "    StructField(\"Age\", FloatType(), True),\n",
    "    StructField(\"Height\", FloatType(), True),\n",
    "    StructField(\"Weight\", FloatType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Games\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Sport\", StringType(), True),\n",
    "    StructField(\"Event\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f479dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners_schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Games\", StringType (), True),\n",
    "    StructField(\"Sport\", StringType(), True),\n",
    "    StructField(\"Event\", StringType(), True),\n",
    "    StructField(\"Medal\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45f28359",
   "metadata": {},
   "outputs": [],
   "source": [
    "noc_schema = StructType([\n",
    "    StructField(\"NOC\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6962dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV file from the local filesystem\n",
    "localdf_athlete = spark.read.csv(local_athlete, header=True, schema=athlete_schema)\n",
    "local_df_winners = spark.read.csv(local_winners, header=True, schema=winners_schema)\n",
    "local_df_noc = spark.read.csv(local_noc, header=True, schema=noc_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35f9d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce to a single file\n",
    "localdf_athlete = localdf_athlete.coalesce(1)\n",
    "local_df_winners = local_df_winners.coalesce(1)\n",
    "local_df_noc = local_df_noc.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c742281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDFS\n",
    "localdf_athlete.write.mode('overwrite').parquet(hdfs_athlete)\n",
    "local_df_winners.write.mode('overwrite').parquet(hdfs_winners)\n",
    "local_df_noc.write.mode('overwrite').parquet(hdfs_noc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a422ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------   reading data from HDFS    --------------------------------\n",
    "\n",
    "# Read the Parquet files from HDFS\n",
    "df_athlete = spark.read.schema(athlete_schema).parquet(hdfs_athlete)\n",
    "df_winners = spark.read.schema(winners_schema).parquet(hdfs_winners)\n",
    "df_noc = spark.read.schema(noc_schema).parquet(hdfs_noc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5099c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------   splitting athlete    ---------------------------------\n",
    "\n",
    "## splitting columns\n",
    "df_athlete = df_athlete.withColumn('Year', split(df_athlete['Games'], ' ').getItem(0)) \\\n",
    "    .withColumn('Season', split(df_athlete['Games'], ' ').getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ad6979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------   formatting athlete into km, m, etc    ---------------------------------\n",
    "\n",
    "# Replacement rules\n",
    "replacements = {\n",
    "    'kilometres': 'km',\n",
    "    'kilometers': 'km',\n",
    "    'metres': 'm',\n",
    "    'meters': 'm'\n",
    "}\n",
    "\n",
    "for old, new in replacements.items():\n",
    "    df_athlete = df_athlete.withColumn(\"Event\", regexp_replace(col(\"Event\"), old, new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56b2ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------   Calculate BMI    ---------------------------------\n",
    "\n",
    "df_athlete = df_athlete.withColumn(\"BMI\", round(col(\"Weight\") / (col(\"Height\") / 100) ** 2, 2))\n",
    "df_athlete = df_athlete.withColumn(\"BMI\", round(col(\"Weight\") / (col(\"Height\") / 100) ** 2, 2).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9023c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------   Join athlete_events with winners    ---------------------------------\n",
    "\n",
    "# Perform an inner join on 'ID' and 'Event' to combine athlete and winner data\n",
    "df_selected = df_athlete.join(df_winners, [\"ID\"], \"inner\") \\\n",
    "    .select(\n",
    "        df_athlete.ID,\n",
    "        df_athlete.Name,\n",
    "        df_winners.Team,\n",
    "        df_winners.Games,\n",
    "        df_winners.Sport,\n",
    "        df_winners.Medal\n",
    "    ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------   Joining athlete_events with noc    ---------------------------------\n",
    "\n",
    "df_joined_noc = df_athlete.join(df_noc, df_athlete.Team == df_noc.region, \"inner\") \\\n",
    "    .select(df_athlete.ID, df_athlete.Name, df_athlete.Sport, df_noc.NOC, df_noc.region).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------  INTO HIVE    ---------------------------------\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS project\")\n",
    "\n",
    "spark.sql(\"USE project\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS athlete_events\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS winners\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS noc\")\n",
    "\n",
    "# ORC table\n",
    "df_athlete.write.mode('overwrite').format(\"orc\").saveAsTable(\"athlete_events\")\n",
    "df_winners.write.mode('overwrite').format(\"orc\").saveAsTable(\"winners\")\n",
    "df_noc.write.mode('overwrite').format(\"orc\").saveAsTable(\"noc\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"SELECT * FROM athlete_events\").show(truncate=False)\n",
    "spark.sql(\"SELECT * FROM winners\").show(truncate=False)\n",
    "spark.sql(\"SELECT * FROM noc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c3353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
